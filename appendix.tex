\appendix
\chapter{Security Proofs of \textnormal{{\fontfamily{qag}\selectfont RevelioBP}}}
\label{scn:appendix}

We present the complete security analysis of the argument of knowledge \proto protocol in the following sections.
Although the soundness and zero-knowledge proofs for \proto have some similarities to the security proofs of the ring signature construction in Omniring \cite{Lai2019}, 
they are not trivial and necessary to describe the \RPlus protocol completely.

\section{Proof of Lemma \ref{thmNoDLBase}}
\label{proofThmNoDLBase}
Since $\textbf{g}_w = (\textbf{g}_w^{\prime} \| \textbf{g}^{\prime})$ and $\textbf{g}^{\prime}$ is generated uniformly, we need only to prove that there is no non-trivial discrete logarithm relation between elements of $\textbf{g}_w^{\prime}$. We show that it is difficult to find $\textbf{l}, \textbf{l}^{\prime} \in \Z^{n+3}$ such that 
$(\textbf{g}_w^{\prime})^{\textbf{l}} = (\textbf{g}_w^{\prime})^{\textbf{l}^{\prime}}$.
Let us denote $\textbf{p} = (p_1, p_2, \dots, p_{n+3})$, $\textbf{l} = (l_1, l_2, \dots, l_{n+3})$ and $\textbf{l}^{\prime} = (l_1^{\prime}, l_2^{\prime}, \dots, l_{n+3}^{\prime})$.

Further, suppose the simulator is given a discrete log problem query $(g_0, X)$ where it wants to compute the discrete log of $X$ with respect to $g_0$. 
The simulator picks randomly scalars $\lambda_i, \rho_1, \rho_2 \rgen \Z_q $ for all $i \in [n+3]$ and sets $g = g_0, h = g^{\rho_1}, g_t = g^{\rho_2}, \text{ and } p_i = X^{\lambda_i}$.
Thus, $C_i = g^{r_i} h^{a_i} = g_0^{(r_i + \rho_1 a_i)}, \text{ and } I_j = g_t^{r_j} h^{a_j} = g_0^{(\rho_2 r_j + a_j)}$.

\begin{align*}
(\textbf{g}_w^{\prime})^{\textbf{l}} &= ((g\|g_t\|\textbf{C}\|I_j^{-u})^w \circ \textbf{p})^{\textbf{l}}\\
&= (g^w p_1 \|g_t^w p_2 \| C_1^w p_3 \| \dots \| C_n^w p_{n+2} \|I_j^{-uw} p_{n+3} )^{\textbf{l}}\\
&= (g^w p_1)^{l_1} (g_t^w p_2)^{l_2} \prod_{i=3}^{n+2} (C_{i-2}^w p_{i})^{l_{i}} (I_j^{-uw} p_{n+3})^{l_{n+3}}\\
&= g^{w l_1} g_t^{w l_2} I_j^{-uwl_{n+3}} \prod_{i=1}^{n} C_{i}^{w l_{i+2}} \prod_{i=1}^{n+3} p_i^{l_i}\\
&= g_0^{w(l_1 + l_2 \rho_2 - u(\rho_2 r_j + a_j)l_{n+3} + \sum_{i=1}^{n}((r_i + \rho_1 a_i)l_{i+2}) )} \cdot X^{\sum_{i=1}^{3} \lambda_i l_i}
\end{align*}
\noindent Thus, we can write $(\textbf{g}_w^{\prime})^{\textbf{l}} = (\textbf{g}_w^{\prime})^{\textbf{l}^{\prime}}$ as $(\textbf{g}_w^{\prime})^{(\textbf{l} - \textbf{l}^{\prime})} = 1$. 
% \vspace{-7pt}
% \begin{align*}
%   (\textbf{g}_w^{\prime})^{(\textbf{l} - \textbf{l}^{\prime})} = 1
% \end{align*}
\begin{multline*}
\implies g_0^{w((l_1-l_1^{\prime}) + (l_2-l_2^{\prime}) \rho_2 - u(\rho_2 r_j + a_j)(l_{n+3}-l_{n+3}^{\prime}))} \\ 
g_0^{w\sum_{i=1}^{n}((r_i + \rho_1 a_i)(l_{i+2}-l_{i+2}^{\prime})) } \cdot X^{\sum_{i=1}^{3} \lambda_i (l_i-l_i^{\prime})} = 1.
\end{multline*}
\noindent Since $\textbf{l}, \textbf{l}^{\prime}$ are distinct, $l_i \neq l_i^{\prime}$ for atleast one $i \in [n+3]$, the simulator is 
able to find a non-trivial discrete log representation of $1$ with respect to $(g_0, X)$ given as
\begin{multline*}
\frac{w}{\sum_{i=1}^{n+3} \lambda_i (l_i-l_i^{\prime})} \cdot \Big[ \left((l_1-l_1^{\prime}) + (l_2-l_2^{\prime}) \rho_2 - u(\rho_2 r_j + a_j)(l_{n+3}-l_{n+3}^{\prime})\right) \\ + \sum_{i=1}^{n}\left((r_i + \rho_1 a_i)(l_{i+2}-l_{i+2}^{\prime})\right) \Big].
\end{multline*}
\begin{flushright}
\qed
\end{flushright}


  
\section{Proof of Theorem \ref{label:thm1} (Perfect SHVZK)}
\label{scnProofTheorem1}
\vspace{-2pt}
Let the verifier challenges be $(u, v, w, y, z, x)$. Simulator $\mathcal{S}$ computes $\hat{I}(u), \textbf{g}_w(u,w)$ as described in \protow. 
We will use $\delta(u,v,y,z)$ to make the dependence of $\delta$ (as defined in Fig. \ref{fig:conVec2}) on the challenges explicit.
% \begin{align}
%     \hat{I} &= \ \textbf{I}^{\vecnb{u}^s},\\
%     \textbf{g}_w &= \left[((g\|g_t\|\textbf{C}\|\hat{I})^{\circ w} \circ \textbf{p}) \| \textbf{g}^{\prime}\right].
% \end{align}
Now, $\mathcal{S}$ samples the following quantities uniformly from respective groups: $S, T_2 \rgen \G, \ \lvec, \rvec \rgen \Z_q^N \ \tau_x, r \rgen \Z_q$.
It then computes the remaining quantities corresponding to the ones which were sent by $\P$ to $\V$ in \proto as follows:
\begin{align}
    \hat{t} &= \langle \lvec, \rvec \rangle,\\
    T_1 &= (g^{\hat{t} - \delta} h^{\tau_x} T_2^{-x^2})^{x^{-1}},\\
    A &= (h^{\prime})^r S^{-x} \textbf{g}_w^{\lvec - \vecnb{\alpha}} \textbf{h}^{\vecnb{\theta}^{\circ -1} \circ \rvec - \vecnb{\beta}}. \label{label:a}
\end{align}

Finally, $\mathcal{S}$ outputs the simulated transcript $(A, S,$ $T_1, T_2, \lvec, \rvec, \hat{t}, \tau_x, r)$. 
Note that since $\lvec, \rvec$ are uniformly sampled from $\Z_q^N$, $\hat{t}$ is uniformly distributed in $\Z_q$. $T_1$ is also uniformly distributed in $\G$ as $g,h, T_2$ are uniformly sampled from $\G$ and the corresponding exponents are also uniformly distributed in $\Z_q$.
From Lemma \ref{thmNoDLBase}, recall that $\textbf{g}_w$ can also be considered as uniformly distributed in $\G^{n+3}$. Thus, $A$ is also uniformly distributed in $\G$ since the generators as well as exponents in the equation for computing $A$ are uniformly distributed in the respective groups.
This implies that all the elements produced by $\mathcal{S}$ and those produced by \proto are identically distributed and also satisfy the verification equations at the end of Figure \ref{fig:protocol_revBP}.
Therefore, the protocol is perfect special honest-verifier zero knowledge. \hfill{\small \qed}
% \begin{flushright}
%   \qed
% \end{flushright}


\section{Proof of Theorem \ref{label:thm2} (Soundness)}
\label{scnProofTheorem2}
\vspace{-3pt}
To prove that \proto has witness-extended emulation, first we state a couple of useful lemmas and a corollary.
Before we proceed, we define some notation and a new system of constraint equations \textsf{CS}. Let $\vecnb{\gamma}_L, \vecnb{\gamma}_R \in \Z_q^N$ be
\begin{align*}
    \vecnb{\gamma}_{L} 
    &\coloneqq 
    (\hspace{0.25mm} \gamma_{L,1} \hspace{0.25mm} \|
    \hspace{0.58mm} \gamma_{L,2} \hspace{0.55mm}\|
    \hspace{0.9mm} \vecnb{\gamma}_{L,3} \hspace{0.9mm}\|
    \hspace{0.6mm} \gamma_{L,4} \hspace{0.6mm}\|
    \hspace{4mm}
    \vecnb{\gamma}_{L,5}
    \hspace{4mm} \|
    \hspace{1mm} \vecnb{\gamma}_{L,6} \hspace{1mm}),\\
    \vecnb{\gamma}_{R} &\coloneqq 
    (\underbrace{\hspace{-0.2mm} \gamma_{R,1} \hspace{-0.2mm}}_{1} \|
    \underbrace{\hspace{-0.2mm} \gamma_{R,2} \hspace{-0.2mm}}_{1}\|
    \underbrace{\hspace{0.1mm} \vecnb{\gamma}_{R,3} \hspace{0.1mm}}_{n}\|
    \underbrace{\hspace{-0.2mm} \gamma_{R,4} \hspace{-0.2mm}}_{1}\|
    \underbrace{\hspace{3.2mm} \vecnb{\gamma}_{R,5} \hspace{3.2mm}}_{sn} \|
    \underbrace{\hspace{0.38mm} \vecnb{\gamma}_{R,6} \hspace{0.38mm}}_{s}),
\end{align*}
where for $i \in \{L,R\}$, $\gamma_{i,j} \in \Z_q$ for $j\in \{1,2,4\}$, $\vecnb{\gamma}_{i,3} \in \Z_q^n$, $\vecnb{\gamma}_{i,6} \in \Z_q^s$ and for some matrices $\Upgamma_L, \Upgamma_R \in \Z_q^{s \times n}$, $\vecnb{\gamma}_{i,5} = \textsl{vec}(\Upgamma_{i}) \in \Z_q^{sn}$.
Define the constraint system \textsf{CS} with parameter $u$ such that $\textsf{CS}(\vecnb{\gamma}_L, \vecnb{\gamma}_R) = 0 \iff$
% \begin{align*}
%   \textsf{CS}(\vecnb{\gamma}_L, \vecnb{\gamma}_R) = 0 \iff
% \end{align*}
\begin{numcases}{}
        \vecnb{\gamma}_{L,5} \circ \vecnb{\gamma}_{R,5} &= $\ \vecnb{0}^{sn}$, \label{eqn:dotprod_TauL_TauR} \\
        \gamma_{L,1} &= $-\langle \vecnb{u}^s, \vecnb{\gamma}_{L,6} \rangle$,\\
        \gamma_{L,2} &= $\langle \vecnb{u}^s, \vecnb{\gamma}_{L,6} \rangle$,\\
        \vecnb{\gamma}_{L,3} &= $\ \vecnb{u}^s \Upgamma_{L}$,\\
        \Upgamma_{L} \vecnb{1}^{n} &= $\ \vecnb{1}^{s}$, \label{eqn:TauLvec} \\
        \gamma_{L,4} &= $1$,\\
        \vecnb{\gamma}_{L,5} &= $-\vecnb{\gamma}_{R,5} + \vecnb{1}^{sn}$. \label{eqn:rel_TauL_TauR}
\end{numcases}
\vspace{-2pt}
\begin{lemma}\label{lemEQimpliesCS}
For a fixed $q > 2^{\lambda}$ and $u,v \in \Z_q$, suppose there exist $\vecnb{\gamma}_L, \vecnb{\gamma}_R \ \in \Z_q^N $ such that we have \textsf{EQ}$(\vecnb{\gamma}_L, \vecnb{\gamma}_R)=0$ for $sn$ different values of $y$ and two different values of $v$, then \textsf{CS}$(\vecnb{\gamma}_L, \vecnb{\gamma}_R)=0$.
\end{lemma}
\vspace{-2pt}
\textit{Proof}: Since \textsf{EQ}$(\vecnb{\gamma}_L, \vecnb{\gamma}_R)=0$ for $sn$ different values of $y$, the following polynomials in $y$ of degree at most $sn-1$ have $sn$ different roots. Thus, all of them must be equal to \textit{zero} polynomials.
\begin{align*}
\langle \vecnb{\gamma}_{L,5} \circ \vecnb{\gamma}_{R,5}, \vecnb{y}^{sn} \rangle = 0 & & \hspace{1cm} \text{by (\ref{ceq0})},\\
    v\cdot \gamma_{L,1} + \gamma_{L,2} + (v-1) \langle \vecnb{\gamma}_{L,6}, \vecnb{u}^s \rangle = 0 & & \hspace{1cm} \text{by (\ref{ceq1})},\\
    \langle \vecnb{\gamma}_{L,3} - \vecnb{u}^s \Upgamma_L, \vecnb{y}^{n} \rangle = 0 & & \hspace{1cm} \text{by (\ref{ceq2})},\\
    (\gamma_{L,4} -1)y^s  + \langle \Upgamma_L \vecnb{1}^n - \vecnb{1}^s, \vecnb{y}^s \rangle = 0 & & \hspace{1cm} \text{by (\ref{ceq3})},\\
    \langle \vecnb{\gamma}_{L,5} + \vecnb{\gamma}_{R,5} - \vecnb{1}^{sn}, \vecnb{y}^{sn} \rangle = 0 & & \hspace{1cm} \text{by (\ref{ceq4})}.
\end{align*}
Furthermore, since \textsf{EQ}$(\vecnb{\gamma}_L, \vecnb{\gamma}_R)=0$ for two different values of $v$ too, the coefficient of $v$ and the constant term in the second equation both must be \textit{zero}.  
By comparing coefficients, we get $\textsf{CS}(\vecnb{\gamma}_L, \vecnb{\gamma}_R)=0$. \hfill{\small \qed}

\begin{lemma}\label{lemTauLBinary}
If \textsf{CS}$(\vecnb{\gamma}_L, \vecnb{\gamma}_R) = 0$ then each row of $\Upgamma_L$ is a unit vector of length $n$.
\end{lemma}
\textit{Proof}: This follows from equations (\ref{eqn:dotprod_TauL_TauR}), (\ref{eqn:TauLvec}) and (\ref{eqn:rel_TauL_TauR}). \hfill{\small \qed}

\begin{corollary}\label{corollaryNoDL}
Assuming the discrete logarithm assumption holds over $\G$, a \textsf{PPT} adversary cannot find a non-trivial discrete logarithm relation between the components of the base $(h^{\prime}\|\textbf{g}_w \| \textbf{h})$
\end{corollary}
\textit{Proof}: Since $(h^{\prime}, \textbf{h})$ are generated uniformly from $\G$, it is infeasible for a PPT adversary to compute its discrete log relation with base vector $\textbf{g}_w$.
Proving that a \textsf{PPT} adversary cannot find a non-trivial discrete log relation between components of $\textbf{g}_w$ follows from Lemma \ref{thmNoDLBase}. \hfill{\small \qed}


With the above lemmas and the corollary, we proceed to construct an extractor $\E$. 
Let $\textsl{pp} \leftarrow \textsl{Setup}(\lambda)$ and $\textsl{stmt, wit} \leftarrow \mathcal{A}(\textsl{pp})$.
The aim of $\E$ is to produce a \textit{valid} transcript and consequently the witness $\textsl{wit}^{\prime}$ corresponding to that transcript.
Since $\E$ has oracle access to $\langle \P^{\star}(\textsl{pp, stmt; wit}), \V(\textsl{pp, stmt}) \rangle$ for any prover $\P^{\star}$, producing a valid transcript
is trivial for $\E$. We hence focus on how $\E$ could extract a valid witness. 

Extractor $\E$ runs $\P^{\star}$ on one value each of $u, v$, 2 different values of $w$, $sn$ different values of $y$,
5 different values of $z$ and 3 different values of $x$. This results in $30\times sn$ transcripts.
$\E$ fixes the values of $(w,y,z)$ and runs $\P^{\star}$ for $x=(x_1,x_2,x_3)$. Let the transcripts for the respective $x$ be $(A, S, T_1, T_2, \tau_{x_i}, r_{x_i}, \lvec_{x_i}, \rvec_{x_i}, \hat{t}_{x_i})$ for $i=1,2,3$. Now $\E$ will extract the discrete logarithm representations of $A, S, T_1, T_2$ using the above transcripts.\\

\noindent
\textbf{Extracting $A$}: Choose $k_i \in \Z_q \text{ for } i=1,2$ such that $\sum_{i=1}^{2} k_i =1$ and $\sum_{i=1}^{2} k_ix_i =0$. From (\ref{label:a}), we have
\begin{gather*}
    A^{k_i} = h^{r_{x_i}k_i} S^{-x k_i} \textbf{g}_w^{k_i \cdot (\lvec_{x_i} - \vecnb{\alpha})} \textbf{h}^{k_i \cdot (\vecnb{\theta}^{\circ -1} \circ \rvec_{x_i} - \vecnb{\beta})} \ \forall i \in \{1,2\}
\end{gather*}
\begin{align*}
    \prod_{i=1}^{2} A^{k_i} = h^{\sum_{i} r_{x_i}k_i} S^{-\sum_{i} k_i x_i}  
    \textbf{g}_w^{ (\sum_{i} k_i \cdot \lvec_{x_i}) - \vecnb{\alpha}(\sum_{i} k_i)}
    \textbf{h}^{(\sum_{i} k_i \vecnb{\theta}^{\circ -1} \circ \rvec_{x_i}) - \vecnb{\beta}(\sum_{i} k_i)}
\end{align*}
\begin{align*}
    & \implies A = h^{\sum_{i} r_{x_i}k_i} 
    \textbf{g}_w^{(\sum_{i} k_i \cdot \lvec_{x_i}) - \vecnb{\alpha}}
    \textbf{h}^{(\sum_{i} k_i \vecnb{\theta}^{\circ -1} \circ \rvec_{x_i}) - \vecnb{\beta}}\\
    & \implies A = h^{r_A^{\prime}} \textbf{g}_w^{\textbf{c}_L^{\prime}} \textbf{h}^{\textbf{c}_R^{\prime}}.
\end{align*}
where $r_A^{\prime} = \sum_{i} r_{x_i}k_i$, $\textbf{c}_L^{\prime} = \left( \sum_{i}  k_i \cdot \lvec_{x_i} \right) - \vecnb{\alpha}$ and $\textbf{c}_R^{\prime} = \left(\sum_{i} k_i \cdot (\vecnb{\theta}^{\circ -1} \circ \rvec_{x_i}) \right) - \vecnb{\beta}$.
Since we have considered the above extraction for a particular $w$ out of the 2 of its values, $r_A^{\prime}, \textbf{c}_L^{\prime}, \textbf{c}_R^{\prime}$ depend on $w$. 
To show that the discrete logarithm representation of $A$ is independent of w, $\E$ repeats the above for a different $w^{\prime}$. In particular, we have $A= h^{r_A^{\prime \prime}} \textbf{g}_{w^{\prime}}^{\textbf{c}_L^{\prime \prime}} \textbf{h}^{\textbf{c}_R^{\prime \prime}}$. 
Now we have two possibly different representations of $A$. Write $\textbf{c}_L^{\prime} = (\textbf{c}_{L,1}^{\prime} \| \textbf{c}_{L,2}^{\prime})$  and $\textbf{c}_L^{\prime \prime} = (\textbf{c}_{L,1}^{\prime \prime} \| \textbf{c}_{L,2}^{\prime \prime})$ of appropriate dimensions. We have
\begin{gather*}
    h^{r_A^{\prime}} \textbf{g}_w^{\textbf{c}_L^{\prime}} \textbf{h}^{\textbf{c}_R^{\prime}} =
    h^{r_A^{\prime \prime}} \textbf{g}_{w^{\prime}}^{\textbf{c}_L^{\prime \prime}} \textbf{h}^{\textbf{c}_R^{\prime \prime}} \implies \\
    1 = h^{r_A^{\prime} - r_A^{\prime \prime}} 
    (g \| g_t \| \textbf{C} \| \hat{I})^{w \cdot \textbf{c}_{L,1}^{\prime} - w^{\prime} \cdot \textbf{c}_{L,1}^{\prime \prime}} 
    (\textbf{p}\|\textbf{g}^{\prime})^{\textbf{c}_{L}^{\prime} - \textbf{c}_{L}^{\prime \prime}}
    \textbf{h}^{\textbf{c}_{R}^{\prime} - \textbf{c}_{R}^{\prime \prime}}.
\end{gather*}

Now, if $r_A^{\prime} \neq r_A^{\prime \prime}, \ \textbf{c}_{L}^{\prime} \neq \textbf{c}_{L}^{\prime \prime}, \ \textbf{c}_{R}^{\prime} \neq \textbf{c}_{R}^{\prime \prime}$, since $(\textbf{p}\| \textbf{g}^{\prime} \| \textbf{h})$ is uniformly chosen after fixing $(g \| g_t \| \textbf{C} \| \hat{I})$, 
we would have violated the discrete logarithm assumption. Thus, $r_A^{\prime} = r_A^{\prime \prime}, \ \textbf{c}_{L}^{\prime} = \textbf{c}_{L}^{\prime \prime}, \ \textbf{c}_{R}^{\prime} = \textbf{c}_{R}^{\prime \prime}$ and letting $\textbf{c}_L^{\prime} = (\xi^{\prime} \| \xi^{\prime \prime} \| \hat{\textbf{e}}^{\prime} \| \psi^{\prime})$,
we get 
% \begin{align}
%     1 &= (g \| g_t \| \textbf{C} \| \hat{I})^{(w-w^{\prime}) \cdot \textbf{c}_L^{\prime}}\nonumber\\
%     1 &= (g \| g_t \| \textbf{C} \| \hat{I})^{\textbf{c}_L^{\prime}} \text{  since } w \neq w^{\prime}\nonumber\\
%     1 &= \big(g^{\xi^{\prime}} \cdot g_t^{\xi^{\prime \prime}} \cdot \textbf{C}^{\hat{\textbf{e}}^{\prime}} \cdot \hat{I}^{\psi^{\prime}} \big) \label{label:wit_1}
% \end{align}
\begin{gather}
1 = (g \| g_t \| \textbf{C} \| \hat{I})^{(w-w^{\prime}) \cdot \textbf{c}_L^{\prime}} \nonumber \\
\implies \hspace{1.8mm} 1 = (g \| g_t \| \textbf{C} \| \hat{I})^{\textbf{c}_L^{\prime}} \text{  since } w \neq w^{\prime} \nonumber \\
\hspace{-14.1mm} \implies \hspace{1.8mm} 1 = g^{\xi^{\prime}} \cdot g_t^{\xi^{\prime \prime}} \cdot \textbf{C}^{\hat{\textbf{e}}^{\prime}} \cdot \hat{I}^{\psi^{\prime}}. \label{label:wit_1}
\end{gather}
We will use equation (\ref{label:wit_1}) in the last part of the proof.\\

\noindent
\textbf{Extracting $S$}: $\E$ samples some $k_1, k_2 \in \Z_q$ such that $\sum_{i}k_i=0$ and $\sum_{i}k_ix_i=1$. From (\ref{label:a}), we have
\begin{gather*}
    S^{x_i} = h^{r_{x_i}} A^{-1} \textbf{g}_w^{\lvec_{x_i} - \vecnb{\alpha}} \textbf{h}^{\vecnb{\theta}^{\circ -1} \circ \rvec_{x_i} - \vecnb{\beta}} \ \forall i \in \{1,2\}
\end{gather*}
\begin{align*}
  \prod_{i=1}^{2}S^{k_ix_i} = h^{\sum_i k_ir_{x_i}} 
    A^{-\sum_i k_i} 
    \textbf{g}_w^{(\sum_i k_i \cdot \lvec_{x_i}) - (\sum_i k_i)\vecnb{\alpha}} 
    \textbf{h}^{( \sum_i k_i \cdot \vecnb{\theta}^{\circ -1} \circ \rvec_{x_i}) - (\sum_i k_i)\vecnb{\beta}}
\end{align*}
\begin{gather}
S = h^{r_S^{\prime}} \textbf{g}_w^{\sum_i k_i \cdot \lvec_{x_i}} \textbf{h}^{\sum_i k_i \cdot \vecnb{\theta}^{\circ -1} \circ \rvec_{x_i}}\\
\implies S = h^{r_S^{\prime}} \textbf{g}_w^{\textbf{s}_L^{\prime}} \textbf{h}^{\textbf{s}_R^{\prime}} 
\end{gather}

For a fixed $w$, the extracted $A, S$ hold for all possible $(x,y,z)$ because otherwise, the discrete log assumption would be violated owing to Corollary \ref{corollaryNoDL} as a non-trivial discrete logarithm representation of $1$ with respect to the base $(h^{\prime}\| \textbf{g}_w \| \textbf{h})$ would be known.

Substituting these expressions of $A, S$ in the expressions for $\lvec, \rvec$ from the protocol, we get
\begin{gather*}
    \lvec_x^{\prime} = \ \textbf{c}_L^{\prime} + \vecnb{\alpha} + \textbf{s}_L^{\prime} \cdot x \ \in \Z_q^N, \\
    \rvec_x^{\prime} = \ \vecnb{\theta} \circ \ (\textbf{c}_R^{\prime} + \textbf{s}_R^{\prime} \cdot x) + \vecnb{\mu} \ \in \Z^N_q.
\end{gather*}

These vectors must also hold for all $(x,y,z)$ because if that was not the case, then we would know a non-trivial discrete logarithm representation of $1$ with respect to the base $(h^{\prime}\| \textbf{g}_w \| \textbf{h})$ due to Corollary \ref{corollaryNoDL}.\\


% \textbf{Extracting $T_1,T_2$}: The verification equation for checking $\hat{t}=t_0+t_1x +t_2x^2$ in the protocol can be rewritten as follows:
% \begin{gather*}
%     g^{\hat{t}}h^{\tau_x} \stackrel{?}{=} g^{\delta} T_1^{x} T_2^{x^2}\\
%     \iff
%     T_1^{x} \stackrel{?}{=} g^{\hat{t}-\delta} h^{\tau_x} T_2^{-x^2} \quad  \text{or} \quad T_2^{x^2} = g^{\hat{t}-\delta} h^{\tau_x} T_1^{-x}
% \end{gather*}

\noindent
\textbf{Extracting $T_1,T_2$}:  $\E$ chooses $k_i \in \Z_q \text{ for } i \in \{1,2,3\}$ such that $\sum_{i=1}^{3}k_i=0$, $\sum_{i=1}^{3}k_ix_i=1$ and $\sum_{i=1}^{3}k_ix_i^2=0$. Thus, we have
\begin{gather*}
    T_1 = \prod_{i=1}^{3} T_1^{k_ix_i} = g^{\sum_{i=1}^{3} k_i \hat{t}_{x_i}} h^{\sum_{i=1}^{3} k_i \tau_{x_i}} = g^{t_1^{\prime}} h^{r_1^{\prime}}.
    % \implies 
    % T_1 = g^{t_1^{\prime}} h^{r_1^{\prime}}
\end{gather*}

Similarly, to extract $T_2$, $\E$ chooses $k_i^{\prime} \in \Z_q \text{ for } i \in \{1,2,3\}$ such that $\sum_{i=1}^{3}k_i^{\prime}=0$, $\sum_{i=1}^{3}k_i^{\prime}x_i=0$ and $\sum_{i=1}^{3}k_i^{\prime}x_i^2=1$. Thus, we have
\begin{gather*}
    T_2 = \prod_{i=1}^{3} T_2^{k_i^{\prime} x_i^2} = g^{\sum_{i=1}^{3} k_i^{\prime} \hat{t}_{x_i}} h^{\sum_{i=1}^{3} k_i^{\prime} \tau_{x_i}} = g^{t_2^{\prime}} h^{r_2^{\prime}}.
    % \implies 
    % T_2 = g^{t_2^{\prime}} h^{r_2^{\prime}}
\end{gather*}

Again, the above expressions for $T_1, T_2$ hold for all $x$, or otherwise we would have obtained a non-trivial discrete logarithm representation of $1$ base $(g\|h)$ violating the discrete logarithm assumption.
Therefore, we have obtained $t_1^{\prime}, t_2^{\prime} \in \Z_q$ as the exponents of $g$ in the extracted $T_1$ and $T_2$ respectively.\\

\noindent
\textbf{Extracting witness}: $\E$ parses $\textbf{c}_L^{\prime}$ as below and outputs the witness $\textsl{wit}^{\prime}$ 
\begin{align*}
    \textbf{c}_L^{\prime} &= (\xi^{\prime} \| \xi^{\prime \prime} \| \hat{\textbf{e}}^{\prime} \| \psi^{\prime}\| \textsl{vec}(\Emat^{\prime}) \| \textbf{r}^{\prime}), \\
    \textsl{wit}^{\prime} &= (\textbf{r}^{\prime}, \textbf{e}_{i_1}^{\prime}, \dots \textbf{e}_{i_s}^{\prime}).
\end{align*}

Finally, what remains to show is that the extracted witness is a valid witness to the statement $\textsl{stmt}$. Using the extracted $t_1^{\prime}, t_2^{\prime}$ we have
\begin{gather*}
    t_{x}^{\prime} = \delta(u,v,y, z) + t_1^{\prime}x + t_2^{\prime}x^2.
\end{gather*}
for all $(x,y,z)$ or else we would violate the DL assumption by having a DL relation between $g,h$. Let
\begin{align*}
    t_0^{\prime} &\coloneqq \delta(u,v,y, z), \\
    l^{\prime}(X) &\coloneqq \ \textbf{c}_L^{\prime} + \vecnb{\alpha} + \textbf{s}_L^{\prime} \cdot X ,
    \\
    r^{\prime}(X) &\coloneqq \ \vecnb{\theta} \circ \ (\textbf{c}_R^{\prime} + \textbf{s}_R^{\prime} \cdot X) + \vecnb{\mu},
    \\
    t^{\prime}(X) &\coloneqq \langle l^{\prime}(X),r^{\prime}(X) \rangle.
\end{align*}
Now, the following polynomial, for all $(y,z)$, has at least 3 roots and hence must be a zero polynomial.
\begin{align*}
    t^{\prime}(X) - (t_0^{\prime} + t_1^{\prime}X + t_2^{\prime}X^2).
\end{align*}
We have $t^{\prime}(X) = t_0^{\prime} + t_1^{\prime}X + t_2^{\prime}X^2 $ and particularly, $t^{\prime}(0) = t_0^{\prime}$. The latter two quantities are given by 
\begin{align}
    t_0^{\prime} &= z^3 \cdot \langle \vecnb{1}^{s+1}, \vecnb{y}^{s+1} \rangle + \langle \vecnb{\alpha}, \vecnb{\mu} \rangle + \langle \vecnb{1}^N, \vecnb{\nu}\rangle, \label{label:t0}\\
    t^{\prime}(0) &= 
    \langle \textbf{c}_L^{\prime}, \vecnb{\theta} \circ \textbf{c}_R^{\prime} \rangle + 
    \langle \textbf{c}_L^{\prime}, \vecnb{\mu} \rangle
    +
    \langle \textbf{c}_R^{\prime}, \vecnb{\theta} \circ \vecnb{\alpha} \rangle
    +
    \langle \vecnb{\alpha}, \vecnb{\mu} \rangle \nonumber \\[2pt]
    &=
    \langle \textbf{c}_L^{\prime}, \vecnb{\theta} \circ \textbf{c}_R^{\prime} \rangle + 
    \langle \textbf{c}_L^{\prime}, \vecnb{\zeta} \rangle
    + 
    \langle \textbf{c}_L^{\prime}, \vecnb{\nu} \rangle
    +
    \langle \textbf{c}_R^{\prime}, \vecnb{\nu} \rangle
    +
    \langle \vecnb{\alpha}, \vecnb{\mu} \rangle \nonumber\\[2pt]
    &=
    \langle \textbf{c}_L^{\prime}, \vecnb{\theta} \circ \textbf{c}_R^{\prime} \rangle + 
    \langle \textbf{c}_L^{\prime}, \vecnb{\zeta} \rangle \ +
    \langle \textbf{c}_L^{\prime}+\textbf{c}_R^{\prime}, \vecnb{\nu} \rangle
    +
    \langle \vecnb{\alpha}, \vecnb{\mu} \rangle, \label{label:tX0}
\end{align}
where $\vecnb{\zeta} = \sum_{i=1}^{3} z^{i} \textbf{v}_i$. Equations (\ref{label:t0}) and (\ref{label:tX0}) imply
\begin{align*}
    z^3 \langle \vecnb{1}^{s+1}, \vecnb{y}^{s+1} \rangle
    &= \langle \textbf{c}_L^{\prime}, \vecnb{\theta} \circ \textbf{c}_R^{\prime} \rangle + 
    \langle \textbf{c}_L^{\prime}, \vecnb{\zeta} \rangle
    +
    \langle \textbf{c}_L^{\prime}-\textbf{c}_R^{\prime} - \vecnb{1}^{t}, \vecnb{\nu} \rangle
    +
    \langle \vecnb{\alpha}, \vecnb{\mu} \rangle
    \\[2pt]
    &= 
    \langle \textbf{c}_L^{\prime}, \textbf{v}_0 \circ \textbf{c}_R^{\prime} \rangle + 
    \sum_{i=1}^{3} z^i \langle \textbf{c}_L^{\prime}, \textbf{v}_i \rangle +
    z^4 \langle \textbf{c}_L^{\prime}+\textbf{c}_R^{\prime} - \vecnb{1}^{N}, \textbf{v}_4 \rangle.
\end{align*}

The above equation holds for $5$ different values of $z$. 
As the equation involves a degree 4 polynomial, the coefficients on both sides must be equal. 
This implies that $\textsf{EQ}(\textbf{c}_L^{\prime}, \textbf{c}_R^{\prime})=0$ for $sn$ different values of $y$ and $2$ values of $v$. 
By Lemma \ref{lemEQimpliesCS}, we have $\textsf{CS}(\textbf{c}_L^{\prime}, \textbf{c}_R^{\prime})=0$. 
Further, Lemma \ref{lemTauLBinary} implies that each row vector of $\Emat^{\prime}$ is a unit vector of length $n$.
Let $\textsl{vec}(\Emat^{\prime}) = (\textbf{e}_{i_1}^{\prime}, \dots, \textbf{e}_{i_s}^{\prime})$ and write
\begin{equation*}
    \xi^{\prime} = -\langle \vecnb{u}^s, \textbf{r}^{\prime} \rangle, \ \xi^{\prime \prime} = \langle \vecnb{u}^s, \textbf{r}^{\prime}\rangle, \
    \psi^{\prime} = 1, \
    \hat{\textbf{e}}^{\prime} = \vecnb{v}^{s}\Emat^{\prime}.
\end{equation*}

Also, let $i_1^{\prime}, i_2^{\prime}, \dots, i_s^{\prime}$ be the indices of the non-zero numbers in vector
$\hat{\textbf{e}}^{\prime}$. We now show that these exponents computed from the extracted witness
$(\textbf{r}^{\prime}, \textbf{e}_{i_1}^{\prime}, \textbf{e}_{i_2}^{\prime}, \dots, \textbf{e}_{i_s}^{\prime} )$ is correct.
From (\ref{label:wit_1}), we have
\begin{align*}
    1 =& \ g^{\xi^{\prime}} \cdot g_t^{\xi^{\prime \prime}} \cdot \textbf{C}^{\hat{\textbf{e}}^{\prime}} \cdot \hat{I}^{\psi^{\prime}}\\
    =& \ g^{-\langle \vecnb{u}^s, \textbf{r}^{\prime} \rangle}\cdot
    g_t^{\langle \vecnb{u}^s, \textbf{r}^{\prime}\rangle}\cdot
    \left(\prod_{j=1}^{s} \textbf{C}^{u^{j - 1} \cdot \textbf{e}_{i_j}^{\prime}}\right) \cdot 
    \left(\prod_{j=1}^{s} I_j^{-u^{j-1}}\right)\\
    =& \ \prod_{j=1}^{s} \left( g^{-r_j^{\prime}} g_t^{r_j^{\prime}} \textbf{C}^{\textbf{e}_{i_j}^{\prime}} I_j^{-1}\right)^{u^{j - 1}}.
\end{align*}

The final equality can be interpreted as an evaluation of an $s$-degree polynomial in the exponent at a random point $u$.
The probability of such an evaluation being zero for a non-zero polynomial is bounded by $\frac{s+1}{q}$, which is negligible since $q > 2^{\lambda}$ 
by Schwartz-Zippel lemma.
% Hence, the probability of this happening when the polynomial is non-zero is bounded by $\frac{s+1}{q}$ which is negligible since $q > 2^{\lambda}$ 
% by Schwartz-Zippel lemma.
Thus, we assume that the polynomial is always \textit{zero}. 
This implies that for all $ j \in [s]$, $g^{-r_j^{\prime}} g_t^{r_j^{\prime}} \textbf{C}^{\textbf{e}_{i_j}^{\prime}} I_j^{-1} = 1$.
%\begin{gather*}
%  \implies g^{-r_j^{\prime}} g_t^{r_j^{\prime}} \textbf{C}^{\textbf{e}_{i_j}^{\prime}} I_j^{-1} = 1 \ \forall j \in [s] \\
%  \therefore \textbf{C}^{\textbf{e}_{i_j}^{\prime}} = g^{r_j^{\prime}} h^{a_j^{\prime}} \text{ and } I_j = g_t^{r_j^{\prime}} h^{a_j^{\prime}}\\
%\end{gather*}
Now the amount $a_j^{\prime}$ can be calculated (after extracting $(r_j^{\prime}, {\textbf{e}^{\prime}}_{i_j})$) by an honest \textsf{PPT} prover (or extractor) since the amount lies in the finite range $\{0,1,\dots, 2^{64}-1\}$.
Therefore, $\textsl{wit}^{\prime}$ is a valid witness corresponding to the statement $\textsl{stmt}$ for the language $\L_{\RBmath}$. \hfill{\small \qed}


\section{Proof of Theorem \ref{label:thm3} (Output Privacy)}
\label{scnProofTheorem3}
We will prove Theorem \ref{label:thm3} by contradiction. We will prove that if there is a \textsf{PPT} distinguisher $\D$ who can succeed in the \texttt{OutputPriv} experiment with probability at least $\frac{1}{2} + \frac{1}{\mathfrak{p}(\lambda)}$ for a polynomial $\mathfrak{p}$, then we can construct a \textsf{PPT} adversary $\E$ who can solve the generalized DDH problem \cite{Bao2003} with success probability at least $\frac{1}{2}+\frac{1}{2\mathfrak{p}(\lambda)}$. This is a contradiction as the generalized DDH problem is equivalent to the DDH problem and the latter is assumed to be hard in the group $\G$.

Let $\E$ be an adversary who is tasked with solving the generalized DDH problem given a tuple $(g_0,g_1, \dots, g_{f(\lambda)}, u_0,u_1, \dots, u_{f(\lambda)}) \in \G^{2f(\lambda)+2}$. $\E$ wants to distinguish between the following two cases:

\pointsStart
\item In the tuple $(g_0,g_1, \dots, g_{f(\lambda)}, u_0,u_1, \dots, u_{f(\lambda)}) \in \G^{2f(\lambda)+2}$, $g_l \rgen \G$, $u_l \rgen \G \ \forall l=0,1,2,\dots, f(\lambda)$.
\item In the tuple $(g_0,g_1, \dots, g_{f(\lambda)}, u_0,u_1, \dots, u_{f(\lambda)})\in \G^{2f(\lambda)+2}$, $g_l \rgen \G$, $u_l = g_l^{r} \ \forall l=0,1,2,\dots, f(\lambda)$ where $r \rgen \Z_q$.
\pointsEnd

Let $\mathfrak{d}=1$ and $\mathfrak{d}=2$ denote the above two cases, which are assumed to be equally likely. $\E$ needs to output its estimate $\mathfrak{d}'$ of $\mathfrak{d}$. To estimate $\mathfrak{d}$ correctly, $\E$ constructs a valid input to the \texttt{OutputPriv} distinguisher $\D$ as follows:

\begin{enumerate}
\item $\E$ sets $g = g_0$ and chooses $h \rgen \G$. It also chooses amounts $a_1, a_2 \rgen V$.
%\item For $l=1,2,\dots, f(\lambda)$, $\E$ informs $\D$ that the bases $g_l$ and $h$ will be used to create the Pedersen commitments in the $l$th \RPlus proof.
\item $\E$ chooses an integer $\mathfrak{b}$ uniformly from $\{1,2\}$. It sets $C_{\mathfrak{b}} = u_0h^{a_{\mathfrak{b}}}$ and chooses the other output uniformly from $\G$.
For $l=1,2,\ldots,f(\lambda)$, $\E$ sets the tags $I_l = u_lh^{a_{\mathfrak{b}}}$.
    
\item For $l=1,2,\ldots,f(\lambda)$, $\E$ creates the $l$th argument \proto using the \textsf{PPT} simulator $\mathcal{S}$ in Appendix \ref{scnProofTheorem2} with $g_t = g_l$, $\textbf{C} = (C_1,C_2)$, and $\textbf{I} = (I_l)$.
\item $\E$ feeds the computed quantities to $\D$ and gets \\[-16pt]
    %\begin{multline*}
    %  \mathfrak{b}^{\prime} = \D(I_1,\ldots,I_{f(\lambda)},\Uppi_{\RBmath}^1,\ldots,\Uppi_{\RBmath}^{f(\lambda)},\\
    %  g_1,\ldots,g_{f(\lambda)},C_1, C_2, a_1, a_2).
    %\end{multline*}
    \begin{align*}
    \mathfrak{b}^{\prime} = \D\left(\left\{ I_j, \Uppi_{\RBmath}^j, g_j \right\}_{j=1}^{f(\lambda)},C_1, C_2, a_1, a_2\right).
    \end{align*}
\item If $\mathfrak{b'} = \mathfrak{b}$, then $\E$ sets $\mathfrak{d}' = 2$. Otherwise, $\mathfrak{d}' = 1$.
\end{enumerate}

The motivation behind this construction is that when $\D$ estimates $\mathfrak{b}$ correctly it could be exploiting some structure in the inputs given to it.
\pointsStart
\item When $\mathfrak{d} = 1$, the $u_0,u_1,\ldots,u_{f(\lambda)}$ components of the tuple given to $\E$ are uniformly distributed. This makes the distribution of $(I_1,I_2,\ldots,I_{f(\lambda)}, C_1, C_2 )$ identical for both $\mathfrak{b}=1$ and $\mathfrak{b}=2$. This in turn makes the distributions of the simulated arguments $\Uppi_{\RBmath}^1,\ldots,\Uppi_{\RBmath}^{f(\lambda)}$ identical for both values of $\mathfrak{b}$. Thus $\D$ can only estimate $\mathfrak{b}$ with a success probability of $\frac{1}{2}$. Thus $\Pr\left[ \mathfrak{b}' =  \mathfrak{b} \mid \mathfrak{d} = 1\right] = \frac{1}{2}.$
%\begin{align}
%  \Pr\left[ \mathfrak{b}' =  \mathfrak{b} \mid \mathfrak{d} = 1\right] = \frac{1}{2}.
%\end{align}
\item When $\mathfrak{d} = 2$, the $u_l = g_l^r$ for all $l=0,1,\ldots,f(\lambda)$. By construction, the vector $(I_1,I_2,\ldots,I_{f(\lambda)}, C_1, C_2 )$ has a distribution which is different for $\mathfrak{b}=1$ and $\mathfrak{b}=2$. More importantly, the input $\E$ feeds to $\D$ is identically distributed to the input $\D$ receives in the \texttt{OutputPriv} experiment. If $\D$ can estimate $\mathfrak{b}$ correctly, then $\E$ bets on the distinguisher $\D$'s ability to win in the \texttt{OutputPriv} experiment and concludes that the tuple it received is a generalized DDH tuple.
\pointsEnd

Clearly, if adversary $\D$ is \textsf{PPT} then so is $\E$. Suppose there is a \textsf{PPT} distinguisher $\D$ which succeeds in the \texttt{OutputPriv} experiment with probability of success which is lower bounded by $\frac{1}{2} + \frac{1}{\mathfrak{p}(\lambda)}$ where $\mathfrak{p}$ is a polynomial. Thus we have
$\Pr\left[ \mathfrak{b}' =  \mathfrak{b} \mid \mathfrak{d} = 2\right] \ge \frac{1}{2} + \frac{1}{\mathfrak{p}(\lambda)}.$
%\begin{align}
%  \Pr\left[ \mathfrak{b}' =  \mathfrak{b} \mid \mathfrak{d} = 2\right] \ge \frac{1}{2} + \frac{1}{\mathfrak{p}(\lambda)}.
%\end{align}

The success probability of $\E$ is given by
%{\scriptsize
\begin{align*}
\Pr[\mathfrak{d}^{\prime}=\mathfrak{d}] &= \frac{1}{2}\Pr[\mathfrak{d}^{\prime} = 1 | \mathfrak{d} = 1] + \frac{1}{2}\Pr[\mathfrak{d}^{\prime} = 2 | \mathfrak{d} = 2], \\
                                        &= \frac{1}{2}\Pr[\mathfrak{b}^{\prime} \neq \mathfrak{b} \mid \mathfrak{d} = 1] + \frac{1}{2}\Pr[\mathfrak{b}^{\prime} = \mathfrak{b} \mid \mathfrak{d} = 2], \\
                                        & \ge \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \left( \frac{1}{2} + \frac{1}{\mathfrak{p}(\lambda)} \right) = \frac{1}{2} + \frac{1}{2\mathfrak{p}(\lambda)}.
\label{eqn:DSuccess_probability}
\end{align*}
%}

Thus, $\E$ succeeds in solving the generalized DDH problem with a probability non-negligibly larger than $\frac{1}{2}$. As a \textsf{PPT} adversary who can solve the generalized DDH problem is equivalent to a \textsf{PPT} adversary solving the classical DDH problem \cite{Bao2003}, we get a contradiction. \hfill{\small \qed}
%It follows that any \textsf{PPT} distinguisher $\D$ in the \texttt{OutputPriv} experiment can only succeed with a probability which is negligibly close to $\frac{1}{2}$. 



% \chapter{Test}